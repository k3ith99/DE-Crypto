{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbe56fb-00f7-448a-91cb-441fd2185c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "# %pycodestyle_off -to turn it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c1f7065-d0cd-470d-ab19-f938e26a6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycodestyle_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cb767a-7b5a-4cba-b24d-0ae3feded6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:65: E231 missing whitespace after ','\n",
      "4:80: E501 line too long (119 > 79 characters)\n",
      "4:93: E231 missing whitespace after ','\n",
      "4:104: E231 missing whitespace after ','\n",
      "4:116: E231 missing whitespace after ','\n",
      "5:80: E501 line too long (127 > 79 characters)\n",
      "5:88: E231 missing whitespace after ','\n",
      "5:105: E231 missing whitespace after ','\n",
      "5:116: E231 missing whitespace after ','\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, month, year,monotonically_increasing_id,row_number,concat, udf,lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType,TimestampNTZType,StringType,DecimalType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.conf import SparkConf\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from binance import Client\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from binance.helpers import date_to_milliseconds, interval_to_milliseconds\n",
    "from binance.exceptions import BinanceRequestException, BinanceAPIException\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dateutil.parser import parse\n",
    "import time\n",
    "import logging\n",
    "import psycopg2\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e599f2a-a8c2-4d5b-8acc-6550f36bd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "MINIO_USER = os.getenv(\"MINIO_ROOT_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "client_binance = Client(API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4023de86-2b14-4e29-90e0-64a949ce864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 18:39:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "3:80: E501 line too long (82 > 79 characters)\n",
      "7:67: E261 at least two spaces before inline comment\n",
      "7:67: E262 inline comment should start with '# '\n",
      "7:80: E501 line too long (101 > 79 characters)\n",
      "7:102: W291 trailing whitespace\n",
      "13:80: E501 line too long (80 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CryptoETL\") \\\n",
    "    .config(\"spark.jars\", \"/Users/hamza/Desktop/projects/postgresql-42.7.5.jar\") \\\n",
    "    .getOrCreate()\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", MINIO_USER)#turn into access key in the future \n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", MINIO_PASSWORD)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457ad682-a432-4de4-b526-4331b68563fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:31: E261 at least two spaces before inline comment\n",
      "2:32: E262 inline comment should start with '# '\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) #set to debug to capture all levels\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "847fe01e-ba2f-417d-8d7e-41fbfaffedfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 18:39:50 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5dce917-2573-401d-918a-13f4bd641df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:21: E231 missing whitespace after ','\n",
      "1:31: E231 missing whitespace after ','\n",
      "1:41: E231 missing whitespace after ','\n",
      "1:51: E231 missing whitespace after ','\n",
      "1:63: E261 at least two spaces before inline comment\n",
      "1:64: E262 inline comment should start with '# '\n",
      "1:80: E501 line too long (98 > 79 characters)\n",
      "4:9: E265 block comment should start with '# '\n",
      "5:19: E251 unexpected spaces around keyword / parameter equals\n",
      "5:21: E251 unexpected spaces around keyword / parameter equals\n",
      "6:19: E251 unexpected spaces around keyword / parameter equals\n",
      "6:21: E251 unexpected spaces around keyword / parameter equals\n"
     ]
    }
   ],
   "source": [
    "cryptos = ['BTCUSDT','ETHUSDT','LTCUSDT','BNBUSDT','DOGEUSDT'] #consider improving maintainability\n",
    "client_minio = Minio(\n",
    "        \"localhost:9000\",  # Make sure you're using port 9000 for the S3 API\n",
    "        #minio_url,\n",
    "        access_key = MINIO_USER,\n",
    "        secret_key = MINIO_PASSWORD,\n",
    "        secure=False  # Disable SSL if you're not using SSL certificates\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a7244c-82a4-468f-b6ae-c44317cdf68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:22: E502 the backslash is redundant between brackets\n",
      "2:37: E251 unexpected spaces around keyword / parameter equals\n",
      "2:39: E251 unexpected spaces around keyword / parameter equals\n",
      "2:50: E231 missing whitespace after ','\n",
      "2:59: E251 unexpected spaces around keyword / parameter equals\n",
      "2:61: E251 unexpected spaces around keyword / parameter equals\n",
      "2:80: E231 missing whitespace after ','\n",
      "2:80: E501 line too long (100 > 79 characters)\n",
      "2:89: E251 unexpected spaces around keyword / parameter equals\n",
      "2:91: E251 unexpected spaces around keyword / parameter equals\n",
      "2:100: E502 the backslash is redundant between brackets\n",
      "3:38: E251 unexpected spaces around keyword / parameter equals\n",
      "3:51: E231 missing whitespace after ','\n",
      "3:60: E251 unexpected spaces around keyword / parameter equals\n",
      "3:62: E251 unexpected spaces around keyword / parameter equals\n",
      "3:76: E231 missing whitespace after ','\n",
      "3:80: E501 line too long (95 > 79 characters)\n",
      "3:85: E251 unexpected spaces around keyword / parameter equals\n",
      "3:95: E502 the backslash is redundant between brackets\n",
      "4:38: E251 unexpected spaces around keyword / parameter equals\n",
      "4:52: E231 missing whitespace after ','\n",
      "4:61: E251 unexpected spaces around keyword / parameter equals\n",
      "4:63: E251 unexpected spaces around keyword / parameter equals\n",
      "4:77: E231 missing whitespace after ','\n",
      "4:80: E501 line too long (96 > 79 characters)\n",
      "4:86: E251 unexpected spaces around keyword / parameter equals\n",
      "4:96: E502 the backslash is redundant between brackets\n",
      "5:38: E251 unexpected spaces around keyword / parameter equals\n",
      "5:47: E231 missing whitespace after ','\n",
      "5:56: E251 unexpected spaces around keyword / parameter equals\n",
      "5:58: E251 unexpected spaces around keyword / parameter equals\n",
      "5:72: E231 missing whitespace after ','\n",
      "5:80: E501 line too long (90 > 79 characters)\n",
      "5:81: E251 unexpected spaces around keyword / parameter equals\n",
      "5:83: E251 unexpected spaces around keyword / parameter equals\n",
      "5:90: E502 the backslash is redundant between brackets\n",
      "6:33: E124 closing bracket does not match visual indentation\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\\\n",
    "                    StructField(name = 'datetime',dataType = TimestampNTZType(),nullable = False), \\\n",
    "                    StructField(name= 'Open Price',dataType = DecimalType(),nullable =False), \\\n",
    "                    StructField(name= 'Close Price',dataType = DecimalType(),nullable =False), \\\n",
    "                    StructField(name= 'Volume',dataType = DecimalType(),nullable = False)\\\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48e4a4d9-bc7b-44fc-b0c8-1de99aaeb73f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o448.load.\n: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:364)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n\tat org.postgresql.Driver.connect(Driver.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:161)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:157)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.net.UnknownHostException: postgres1\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:751)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError cleaning data:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,stack_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m read_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM crypto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m df_crypto \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://postgres1:5432/crypto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_crypto_id\u001b[39m(df,df_crypto,crypto,currency):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o448.load.\n: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:364)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n\tat org.postgresql.Driver.connect(Driver.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:161)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:157)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.net.UnknownHostException: postgres1\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:751)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "def parquet_to_df(client,crypto,schema):\n",
    "    #read from parquet from minio and combines into dataframe\n",
    "    try:\n",
    "        objects = client.list_objects(\"binancedata\", prefix=f\"{crypto}/Daily\", recursive=True)\n",
    "        filenames = [obj.object_name for obj in objects]\n",
    "        filenames = [f for f in filenames if \"_SUCCESS\" not in f]\n",
    "        df = spark.createDataFrame(data = [],schema = schema)\n",
    "        for file in filenames:\n",
    "            df_parquet = spark.read.parquet(f\"s3a://binancedata/{file}\")\n",
    "            df = df.union(df_parquet)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"error reading parquet from minio: {e}\", stack_info=True, exc_info=True)\n",
    "\n",
    "def data_cleaning(df):\n",
    "    try:\n",
    "        df_null = df.na.drop(how = 'any',subset = ['datetime'])\n",
    "        df_renamed = df_null.withColumnsRenamed({'Open Price':'open',\n",
    "                                'Close Price':'close',\n",
    "                                'Volume':'volume'})\n",
    "        df_duplicated = df_renamed.dropDuplicates()\n",
    "        df_duplicated = df_duplicated.withColumn(\"open\", col(\"open\").cast(DecimalType(10, 5))) \\\n",
    "                             .withColumn(\"close\", col(\"close\").cast(DecimalType(10, 5))) \\\n",
    "                             .withColumn(\"volume\", col(\"volume\").cast(DecimalType(20, 5)))\n",
    "\n",
    "        return df_duplicated\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning data:{e}\",stack_info=True,exc_info=True)\n",
    "        \n",
    "read_sql = \"SELECT * FROM crypto\"\n",
    "df_crypto = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres1:5432/crypto\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .option(\"query\", read_sql)\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "    .load()\n",
    "\n",
    "def add_crypto_id(df,df_crypto,crypto,currency):\n",
    "    try:\n",
    "        df_crypto = df_crypto.withColumn(\"trading pair\", concat(df_crypto.ticker, lit(currency)))\n",
    "        #obtain the crypto from concatenating currency and ticker\n",
    "        df_crypto = df_crypto.filter(col(\"trading pair\") == crypto)\n",
    "        crypto_id = df_crypto.collect()[0]['id']\n",
    "        df_id = df.withColumn(\"crypto_id\",lit(crypto_id))\n",
    "        return df_id\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error adding crypto id:{e}\",stack_info=True,exc_info=True)\n",
    "    return df_id\n",
    "\n",
    "def generate_time_id(dt_value):\n",
    "    #hard coded\n",
    "    ts = dt_value.strftime(\"%Y%m%d\")\n",
    "    return int(ts)\n",
    "\n",
    "def add_time_id(generate_time_id,df):\n",
    "    try:\n",
    "        dt_udf = udf(generate_time_id,IntegerType())\n",
    "        df_with_udf = df.withColumn(\"time_id\", dt_udf(df[\"datetime\"]))\n",
    "        #df_time = df_time.withColumnRenamed('time_id','id')\n",
    "        return df_with_udf\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error adding time id:{e}\",stack_info=True,exc_info=True)\n",
    "\n",
    "def upload_time(df):\n",
    "    #need to futureproof\n",
    "    df_year = df.withColumn(\"year\", year(df[\"datetime\"]))\n",
    "    df_month =df_year.withColumn(\"month\", month(df_year[\"datetime\"]))\n",
    "    df_day = df_month.withColum(\"day\", day(df_month[\"datetime\"])\n",
    "    df_time_filtered = df_day.select(['time_id','datetime','year','month','day'])\n",
    "    df_time = df_time_filtered.withColumnRenamed('time_id','id')\n",
    "    try:\n",
    "        df_time.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres1:5432/crypto\") \\\n",
    "        .option(\"dbtable\", \"time\") \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"postgres\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\")\\\n",
    "        .save()\n",
    "        logger.info(\"Successfully uploaded to time table\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading to time table:{e}\",stack_info=True,exc_info=True)\n",
    "\n",
    "def upload_price(df):\n",
    "    df_filtered = df.select(['crypto_id','time_id','open','close','volume'])\n",
    "    logger.info(df_filtered.show(5))\n",
    "    try:\n",
    "        df_filtered.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres1:5432/crypto\") \\\n",
    "        .option(\"dbtable\", \"price\") \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"postgres\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\")\\\n",
    "        .save()\n",
    "        logger.info(\"Successfully uploaded to price table\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading to price table:{e}\",stack_info=True,exc_info=True)\n",
    "        \n",
    "def monthly_transform(symbol,currency):\n",
    "    df = parquet_to_df(client = client_minio,crypto = symbol,schema = schema)\n",
    "    df_cleaned = data_cleaning(df)\n",
    "    df_id = add_crypto_id(df_cleaned,df_crypto,symbol,currency)\n",
    "    df_time_id =add_time_id(generate_time_id,df_id)\n",
    "    logger.info(df_time_id.show())\n",
    "    upload_time(df_time_id)\n",
    "    upload_price(df_time_id)\n",
    "    logger.info(f\"Data successfully transformed and loaded for {symbol}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e12c856-59c4-46de-9ad8-f8916d4be426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 18:51:56 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "df = parquet_to_df(client = client_minio,crypto = \"BNBUSDT\",schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57a8b016-2802-4801-a5b6-283c46e9fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = data_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7e92533-ce26-4c5d-a713-426c3c923567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = add_crypto_id(df_cleaned,df_crypto,\"BNBUSDT\",\"USDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "697d89c7-4426-40bd-b0ee-e29fd7f27082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_id =add_time_id(generate_time_id,df_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de2dc8-d610-4a52-bef9-ece12d49d430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
