{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2610a2d-8b5a-4e7a-ae7b-a53bea4fca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "# %pycodestyle_off -to turn it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "176ccd7b-e558-4275-a61f-9ea5b537ff64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, month, year,monotonically_increasing_id,row_number\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType,TimestampNTZType,StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.conf import SparkConf\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from binance import Client\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from binance.helpers import date_to_milliseconds, interval_to_milliseconds\n",
    "from binance.exceptions import BinanceRequestException, BinanceAPIException\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dateutil.parser import parse\n",
    "import time\n",
    "import logging\n",
    "import psycopg2\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d85afa7-52c7-4ca8-a5a1-94680a141540",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "MINIO_USER = os.getenv(\"MINIO_ROOT_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "client_binance = Client(API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ff7f05-169f-4a3c-9aa2-879cb8d4ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:1: E265 block comment should start with '# '\n",
      "2:1: E265 block comment should start with '# '\n",
      "2:80: E501 line too long (265 > 79 characters)\n",
      "3:1: W391 blank line at end of file\n"
     ]
    }
   ],
   "source": [
    "#conf = SparkConf()  # create the configuration\n",
    "#conf.set(\"spark.jars\", \"/Users/hamza/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/jars/Users/hamza/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/jars/postgresql-42.7.5.jar\")  # set the spark.jars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "443a3384-b5c7-4824-9bdd-cd0093557942",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCryptoETL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get the SparkContext from the SparkSession\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CryptoETL\") \\\n",
    "    .config(\"spark.jars\", \"/Users/hamza/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/jars/Users/hamza/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/jars/postgresql-42.7.5.jar\") \\\n",
    "    .getOrCreate()\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", MINIO_USER)#turn into access key in the future \n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", MINIO_PASSWORD)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a0ec56-8517-42d9-ad4b-6aa969df0649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:1: E265 block comment should start with '# '\n",
      "2:1: E265 block comment should start with '# '\n",
      "3:1: E265 block comment should start with '# '\n",
      "4:1: E265 block comment should start with '# '\n",
      "5:1: E265 block comment should start with '# '\n",
      "6:1: E265 block comment should start with '# '\n"
     ]
    }
   ],
   "source": [
    "#read data for 1 crypto\n",
    "#schema check/validation need to do this, create spark df using schema struct\n",
    "#implement transformations\n",
    "#add unique id for crypto,price,time\n",
    "#split into 3 tables\n",
    "#upload to postgres db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c43d2f8-5234-4968-83fb-0b2cd57e11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:31: E261 at least two spaces before inline comment\n",
      "2:32: E262 inline comment should start with '# '\n",
      "6:1: W391 blank line at end of file\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) #set to debug to capture all levels\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "logger.propagate = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb41969-b14c-446a-a285-bad83f6241ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c314b1a-efc6-48ad-9e52-4e33a35f7be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:21: E231 missing whitespace after ','\n",
      "1:31: E231 missing whitespace after ','\n",
      "1:41: E231 missing whitespace after ','\n",
      "1:51: E231 missing whitespace after ','\n",
      "4:9: E265 block comment should start with '# '\n",
      "5:19: E251 unexpected spaces around keyword / parameter equals\n",
      "5:21: E251 unexpected spaces around keyword / parameter equals\n",
      "6:19: E251 unexpected spaces around keyword / parameter equals\n",
      "6:21: E251 unexpected spaces around keyword / parameter equals\n"
     ]
    }
   ],
   "source": [
    "cryptos = ['BTCUSDT','ETHUSDT','LTCUSDT','BNBUSDT','DOGEUSDT']\n",
    "client_minio = Minio(\n",
    "        \"localhost:9000\",  # Make sure you're using port 9000 for the S3 API\n",
    "        #minio_url,\n",
    "        access_key = MINIO_USER,\n",
    "        secret_key = MINIO_PASSWORD,\n",
    "        secure=False  # Disable SSL if you're not using SSL certificates\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1e80e8-32f9-49d4-b77f-d7e57b734a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'client_minio.fget_object(bucket_name = \"binancedata\", \\n                         object_name =\"BNBUSDT/Monthly/year=2019/month=1/part-00000-c9e58e49-1c79-4e98-b1a7-f0611e7acc19.c000.snappy.parquet\", \\n                         file_path =\"./downloaded.parquet\"\\n                        )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:57: W291 trailing whitespace\n",
      "2:80: E501 line too long (142 > 79 characters)\n",
      "2:143: W291 trailing whitespace\n",
      "5:1: E265 block comment should start with '# '\n",
      "6:1: E265 block comment should start with '# '\n",
      "6:80: E501 line too long (114 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "'''client_minio.fget_object(bucket_name = \"binancedata\", \n",
    "                         object_name =\"BNBUSDT/Monthly/year=2019/month=1/part-00000-c9e58e49-1c79-4e98-b1a7-f0611e7acc19.c000.snappy.parquet\", \n",
    "                         file_path =\"./downloaded.parquet\"\n",
    "                        )'''\n",
    "#binancedata/BNBUSDT/Monthly/year=2019/month=1\n",
    "#binancedata/BNBUSDT/Monthly/year=2019/month=1/part-00000-c9e58e49-1c79-4e98-b1a7-f0611e7acc19.c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19fd25a1-c5e7-462e-a072-1b846e26ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:80: E501 line too long (84 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "objects = client_minio.list_objects(\"binancedata\", prefix=\"BNBUSDT\", recursive=True)\n",
    "filenames = [obj.object_name for obj in objects]\n",
    "filenames = [f for f in filenames if \"_SUCCESS\" not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f94ef50-3582-4690-bd26-c9f9f4cfa976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'client_minio.fget_object(bucket_name = \"binancedata\", \\n                         object_name =filenames[0], \\n                         file_path =local_file_path\\n                        )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:57: W291 trailing whitespace\n",
      "2:52: W291 trailing whitespace\n"
     ]
    }
   ],
   "source": [
    "'''client_minio.fget_object(bucket_name = \"binancedata\", \n",
    "                         object_name =filenames[0], \n",
    "                         file_path =local_file_path\n",
    "                        )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e25b66d-fba0-4d88-923d-bf798ef318c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"schema = StructType([                    StructField(name = 'Date',dataType = TimestampNTZType(),nullable = False),                     StructField(name= 'Open Price',dataType = LongType(),nullable =False),                     StructField(name= 'Close Price',dataType = LongType(),nullable =False),                     StructField(name= 'Volume',dataType = LongType(),nullable = False)                                ])\\n                    \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:80: E501 line too long (96 > 79 characters)\n",
      "3:80: E501 line too long (92 > 79 characters)\n",
      "4:80: E501 line too long (93 > 79 characters)\n",
      "5:80: E501 line too long (87 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "'''schema = StructType([\\\n",
    "                    StructField(name = 'Date',dataType = TimestampNTZType(),nullable = False), \\\n",
    "                    StructField(name= 'Open Price',dataType = LongType(),nullable =False), \\\n",
    "                    StructField(name= 'Close Price',dataType = LongType(),nullable =False), \\\n",
    "                    StructField(name= 'Volume',dataType = LongType(),nullable = False)\\\n",
    "                                ])\n",
    "                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ae0b85-5911-4b9a-b0be-f2450bd28aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 14:17:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "1:80: E501 line too long (150 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "df_bnb = spark.read.parquet(\"s3a://binancedata/BNBUSDT/Monthly/year=2019/month=3/part-00000-4770a452-7828-40a3-b901-c42eda5f9c00.c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baa4b2bf-4c00-4d74-be8a-29071a5a8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----------+------------------+\n",
      "|           datetime| Open Price|Close Price|            Volume|\n",
      "+-------------------+-----------+-----------+------------------+\n",
      "|2019-03-01 00:00:00|10.27770000|17.53760000|115379333.44000000|\n",
      "+-------------------+-----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bnb.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57f9b6b8-186e-4358-a864-c75403755c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:1: W293 blank line contains whitespace\n",
      "5:1: W293 blank line contains whitespace\n",
      "6:1: W293 blank line contains whitespace\n"
     ]
    }
   ],
   "source": [
    "for file in filenames:\n",
    "    df = spark.read.parquet(f\"s3a://binancedata/{file}\")\n",
    "    df_bnb = df_bnb.union(df)\n",
    "                            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4237a394-3060-46c0-b060-19502ed14da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:1: E265 block comment should start with '# '\n",
      "2:1: E265 block comment should start with '# '\n"
     ]
    }
   ],
   "source": [
    "#do transformations when its in spark df\n",
    "#push to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13916bf5-2fc0-4f96-ba24-596c6a55ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----------+------------------+\n",
      "|           datetime| Open Price|Close Price|            Volume|\n",
      "+-------------------+-----------+-----------+------------------+\n",
      "|2019-03-01 00:00:00|10.27770000|17.53760000|115379333.44000000|\n",
      "|2019-01-01 00:00:00| 6.11390000| 6.22000000| 72126339.77000000|\n",
      "|2019-10-01 01:00:00|15.84930000|19.90990000| 53481014.91000000|\n",
      "|2019-11-01 00:00:00|19.90870000|15.71180000| 46441747.10000000|\n",
      "|2019-12-01 00:00:00|15.70300000|13.71610000| 38189582.55000000|\n",
      "+-------------------+-----------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bnb.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71835cff-90d7-4c32-a465-4490ea9d4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:5: E265 block comment should start with '# '\n",
      "4:5: E265 block comment should start with '# '\n",
      "5:5: E265 block comment should start with '# '\n",
      "7:1: W293 blank line contains whitespace\n"
     ]
    }
   ],
   "source": [
    "def data_cleaning():\n",
    "    pass\n",
    "    #drop duplicates\n",
    "    #convert data types\n",
    "    #drop null values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "236ac2e1-f8c2-4f85-ba6e-64fd6780e5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:5: E265 block comment should start with '# '\n"
     ]
    }
   ],
   "source": [
    "def df_to_table():\n",
    "    pass\\\n",
    "    #read from postgres\n",
    "    #convert table to df and join with dataframe\n",
    "    #check if id\n",
    "    #convert dataframe to sql tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdcde5ab-c2cf-45c6-bc57-bd2a9507724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[datetime: timestamp, Open Price: string, Close Price: string, Volume: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 14:17:27 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df_bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d9ac523-cee9-4e26-9c2e-4b8a823884fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    database=\"crypto\",  # Switch to the 'crypto' database\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4baed1e4-db04-4e48-8bba-860d3460ae09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 18:37:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/04/20 18:37:39 INFO SharedState: Warehouse path is 'file:/Users/hamza/Desktop/projects/DE-Crypto/src/spark-warehouse'.\n",
      "25/04/20 18:37:40 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "crypto_dict = [{\"name\":\"Bitcoin\",\"ticker\":\"BTC\"},\n",
    "               {\"name\":\"Ethereum\",\"ticker\":\"ETH\"},\n",
    "               {\"name\":\"Litecoin\",\"ticker\":\"LTC\"},\n",
    "               {\"name\":\"Dogecoin\",\"ticker\":\"DOGE\"},\n",
    "               {\"name\":\"BNB\",\"ticker\":\"BNB\"}\n",
    "              ]\n",
    "crypto_df = spark.createDataFrame(crypto_dict)\n",
    "#crypto_df = crypto_df.withColumn('id',monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d237af4a-aa3a-4959-9f30-be85185c6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.orderBy(\"name\")\n",
    "\n",
    "# Add a sequential ID column starting from 10\n",
    "crypto_df = crypto_df.withColumn(\"id\", row_number().over(windowSpec) + 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e81b1e-a9ae-4ee9-95fe-8ea84ca87aab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 18:37:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/20 18:37:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/20 18:37:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/20 18:37:44 INFO CodeGenerator: Code generated in 57.7985 ms\n",
      "25/04/20 18:37:44 INFO CodeGenerator: Code generated in 6.4565 ms\n",
      "25/04/20 18:37:44 INFO CodeGenerator: Code generated in 4.146917 ms\n",
      "25/04/20 18:37:44 INFO SparkContext: Starting job: head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1\n",
      "25/04/20 18:37:44 INFO DAGScheduler: Registering RDD 7 (head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1) as input to shuffle 0\n",
      "25/04/20 18:37:44 INFO DAGScheduler: Got job 0 (head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1) with 1 output partitions\n",
      "25/04/20 18:37:44 INFO DAGScheduler: Final stage: ResultStage 1 (head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1)\n",
      "25/04/20 18:37:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "25/04/20 18:37:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "25/04/20 18:37:44 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1), which has no missing parents\n",
      "25/04/20 18:37:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.1 KiB, free 434.4 MiB)\n",
      "25/04/20 18:37:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.9 KiB, free 434.4 MiB)\n",
      "25/04/20 18:37:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hamzas-laptop:50933 (size: 9.9 KiB, free: 434.4 MiB)\n",
      "25/04/20 18:37:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/20 18:37:45 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/04/20 18:37:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks resource profile 0\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (hamzas-laptop, executor driver, partition 0, PROCESS_LOCAL, 8968 bytes) \n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (hamzas-laptop, executor driver, partition 1, PROCESS_LOCAL, 9011 bytes) \n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (hamzas-laptop, executor driver, partition 2, PROCESS_LOCAL, 8968 bytes) \n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (hamzas-laptop, executor driver, partition 3, PROCESS_LOCAL, 9012 bytes) \n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (hamzas-laptop, executor driver, partition 4, PROCESS_LOCAL, 9012 bytes) \n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (hamzas-laptop, executor driver, partition 5, PROCESS_LOCAL, 8968 bytes) \n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (hamzas-laptop, executor driver, partition 6, PROCESS_LOCAL, 9013 bytes) \n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (hamzas-laptop, executor driver, partition 7, PROCESS_LOCAL, 9003 bytes) \n",
      "25/04/20 18:37:45 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "25/04/20 18:37:45 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "25/04/20 18:37:45 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "25/04/20 18:37:45 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "25/04/20 18:37:45 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "25/04/20 18:37:45 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "25/04/20 18:37:45 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "25/04/20 18:37:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/04/20 18:37:45 INFO CodeGenerator: Code generated in 2.955334 ms\n",
      "25/04/20 18:37:45 INFO CodeGenerator: Code generated in 4.82525 ms  (0 + 8) / 8]\n",
      "25/04/20 18:37:45 INFO PythonRunner: Times: total = 311, boot = 305, init = 5, finish = 1\n",
      "25/04/20 18:37:45 INFO PythonRunner: Times: total = 302, boot = 299, init = 3, finish = 0\n",
      "25/04/20 18:37:45 INFO PythonRunner: Times: total = 298, boot = 294, init = 4, finish = 0\n",
      "25/04/20 18:37:45 INFO PythonRunner: Times: total = 298, boot = 296, init = 2, finish = 0\n",
      "25/04/20 18:37:45 INFO PythonRunner: Times: total = 319, boot = 315, init = 3, finish = 1\n",
      "25/04/20 18:37:45 INFO PythonRunner: Times: total = 323, boot = 321, init = 2, finish = 0\n",
      "25/04/20 18:37:45 INFO PythonRunner: Times: total = 334, boot = 331, init = 2, finish = 1\n",
      "25/04/20 18:37:45 INFO PythonRunner: Times: total = 341, boot = 338, init = 3, finish = 0\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 3379 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 3465 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 3465 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 3379 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3336 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 3465 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 3465 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 3465 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 410 ms on hamzas-laptop (executor driver) (1/8)\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 416 ms on hamzas-laptop (executor driver) (2/8)\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 410 ms on hamzas-laptop (executor driver) (3/8)\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 410 ms on hamzas-laptop (executor driver) (4/8)\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 411 ms on hamzas-laptop (executor driver) (5/8)\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 412 ms on hamzas-laptop (executor driver) (6/8)\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 411 ms on hamzas-laptop (executor driver) (7/8)\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 412 ms on hamzas-laptop (executor driver) (8/8)\n",
      "25/04/20 18:37:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/04/20 18:37:45 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50934\n",
      "25/04/20 18:37:45 INFO DAGScheduler: ShuffleMapStage 0 (head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1) finished in 0.735 s\n",
      "25/04/20 18:37:45 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/20 18:37:45 INFO DAGScheduler: running: Set()\n",
      "25/04/20 18:37:45 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "25/04/20 18:37:45 INFO DAGScheduler: failed: Set()\n",
      "25/04/20 18:37:45 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1), which has no missing parents\n",
      "25/04/20 18:37:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.9 KiB, free 434.3 MiB)\n",
      "25/04/20 18:37:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 434.3 MiB)\n",
      "25/04/20 18:37:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hamzas-laptop:50933 (size: 15.6 KiB, free: 434.4 MiB)\n",
      "25/04/20 18:37:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/20 18:37:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/20 18:37:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8) (hamzas-laptop, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "25/04/20 18:37:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)\n",
      "25/04/20 18:37:45 INFO ShuffleBlockFetcherIterator: Getting 5 (432.0 B) non-empty blocks including 5 (432.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/20 18:37:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/04/20 18:37:45 INFO CodeGenerator: Code generated in 4.887417 ms\n",
      "25/04/20 18:37:45 INFO CodeGenerator: Code generated in 6.534291 ms\n",
      "25/04/20 18:37:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hamzas-laptop:50933 in memory (size: 9.9 KiB, free: 434.4 MiB)\n",
      "25/04/20 18:37:45 INFO CodeGenerator: Code generated in 2.581125 ms\n",
      "25/04/20 18:37:45 INFO CodeGenerator: Code generated in 2.08075 ms\n",
      "25/04/20 18:37:45 INFO CodeGenerator: Code generated in 2.151584 ms\n",
      "25/04/20 18:37:45 INFO CodeGenerator: Code generated in 2.627917 ms\n",
      "25/04/20 18:37:45 INFO Executor: Finished task 0.0 in stage 1.0 (TID 8). 4537 bytes result sent to driver\n",
      "25/04/20 18:37:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 63 ms on hamzas-laptop (executor driver) (1/1)\n",
      "25/04/20 18:37:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/04/20 18:37:45 INFO DAGScheduler: ResultStage 1 (head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1) finished in 0.068 s\n",
      "25/04/20 18:37:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/20 18:37:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/04/20 18:37:45 INFO DAGScheduler: Job 0 finished: head at /var/folders/h2/18kt26x549bcsnv5p3yl229w0000gn/T/ipykernel_58676/2854340515.py:1, took 0.819860 s\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='BNB', ticker='BNB', id=10),\n",
       " Row(name='Bitcoin', ticker='BTC', id=11),\n",
       " Row(name='Dogecoin', ticker='DOGE', id=12),\n",
       " Row(name='Ethereum', ticker='ETH', id=13),\n",
       " Row(name='Litecoin', ticker='LTC', id=14)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f842c003-146b-4126-80d9-46df5db8acef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o82.save.\n: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n\tat org.postgresql.Driver.connect(Driver.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:161)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:157)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:549)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:592)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:751)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)\n\t... 50 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcrypto_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://localhost:5432/crypto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrypto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o82.save.\n: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:352)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n\tat org.postgresql.Driver.connect(Driver.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:161)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:157)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:549)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:592)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:751)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:260)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:121)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)\n\t... 50 more\n"
     ]
    }
   ],
   "source": [
    "crypto_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/crypto\") \\\n",
    "    .option(\"dbtable\", \"crypto\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0cf640-c538-40b1-923b-fed12aa8113b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cursor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(sql_tables)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTables Crypto, Time and Price have been created successfully !!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Closing the connection\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cursor' is not defined"
     ]
    }
   ],
   "source": [
    "cursor.execute(sql_tables)\n",
    "print(\"Tables Crypto, Time and Price have been created successfully !!\")\n",
    "\n",
    "# Closing the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f8ae31-6779-44fa-9c62-22df392d977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crypto_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63aae260-c63a-4c9b-896d-b6b09f233045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:1: W391 blank line at end of file\n"
     ]
    }
   ],
   "source": [
    "crypto_df = crypto_df.drop(\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c6ab7-f72c-40a6-bcc2-054c22cc8b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
