{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbe56fb-00f7-448a-91cb-441fd2185c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "# %pycodestyle_off -to turn it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cb767a-7b5a-4cba-b24d-0ae3feded6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/23 17:21:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/hamza/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:80: E501 line too long (84 > 79 characters)\n",
      "11:18: E231 missing whitespace after ','\n",
      "12:1: E402 module level import not at top of file\n",
      "20:1: E402 module level import not at top of file\n",
      "21:1: E402 module level import not at top of file\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, TimestampNTZType, DecimalType\n",
    "from binance import Client\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import logging\n",
    "import sys\n",
    "import numpy\n",
    "sys.path.insert(0,'./pipelines/shared')\n",
    "from utils import (\n",
    "    parquet_to_df,\n",
    "    data_cleaning,\n",
    "    add_crypto_id,\n",
    "    add_time_id,\n",
    "    upload_time,\n",
    "    upload_price,\n",
    ")\n",
    "import pyspark.pandas as ps\n",
    "import matplotlib as plt\n",
    "load_dotenv(dotenv_path=\"./pipelines/shared/main.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e599f2a-a8c2-4d5b-8acc-6550f36bd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "MINIO_USER = os.getenv(\"MINIO_ROOT_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "client_binance = Client(API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4023de86-2b14-4e29-90e0-64a949ce864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 17:21:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "3:80: E501 line too long (82 > 79 characters)\n",
      "7:67: E261 at least two spaces before inline comment\n",
      "7:67: E262 inline comment should start with '# '\n",
      "7:80: E501 line too long (101 > 79 characters)\n",
      "7:102: W291 trailing whitespace\n",
      "13:80: E501 line too long (80 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CryptoETL\") \\\n",
    "    .config(\"spark.jars\", \"/Users/hamza/Desktop/projects/postgresql-42.7.5.jar\") \\\n",
    "    .getOrCreate()\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", MINIO_USER)#turn into access key in the future \n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", MINIO_PASSWORD)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457ad682-a432-4de4-b526-4331b68563fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:31: E261 at least two spaces before inline comment\n",
      "2:32: E262 inline comment should start with '# '\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) #set to debug to capture all levels\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "847fe01e-ba2f-417d-8d7e-41fbfaffedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db13ab6-33e9-4a1b-84d9-f6c0dacd306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:80: E501 line too long (82 > 79 characters)\n",
      "13:80: E501 line too long (80 > 79 characters)\n",
      "21:1: E265 block comment should start with '# '\n"
     ]
    }
   ],
   "source": [
    "client_minio = Minio(\n",
    "    \"localhost:9000\",  # Make sure you're using port 9000 for the S3 API\n",
    "    # minio_url,\n",
    "    access_key=MINIO_USER,\n",
    "    secret_key=MINIO_PASSWORD,\n",
    "    secure=False,  # Disable SSL if you're not using SSL certificates\n",
    ")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(name=\"datetime\", dataType=TimestampNTZType(), nullable=False),\n",
    "        StructField(name=\"Open Price\", dataType=DecimalType(), nullable=False),\n",
    "        StructField(name=\"Close Price\", dataType=DecimalType(), nullable=False),\n",
    "        StructField(name=\"Volume\", dataType=DecimalType(), nullable=False),\n",
    "    ]\n",
    ")\n",
    "# doesnt work for parquet files, schema inferred from that instead\n",
    "\n",
    "\n",
    "read_sql = \"SELECT * FROM crypto\"\n",
    "#localhost for local development and container ip for containers\n",
    "df_crypto = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/crypto\")\n",
    "    .option(\"user\", \"postgres\")\n",
    "    .option(\"password\", \"postgres\")\n",
    "    .option(\"query\", read_sql)\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a2ff75-9ba5-4fd5-99c2-f84b468daaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 17:21:15 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "1:80: E501 line too long (99 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "df = parquet_to_df(client_minio=client_minio, timeframe=\"monthly\", crypto=\"BTCUSDT\", schema=schema)\n",
    "df_cleaned = data_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49284749-85f9-46be-95a3-e693d1f6a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:1: E265 block comment should start with '# '\n",
      "1:80: E501 line too long (83 > 79 characters)\n",
      "2:1: E265 block comment should start with '# '\n",
      "3:1: E265 block comment should start with '# '\n",
      "4:1: E265 block comment should start with '# '\n",
      "5:1: E265 block comment should start with '# '\n",
      "6:1: E265 block comment should start with '# '\n",
      "7:1: E265 block comment should start with '# '\n"
     ]
    }
   ],
   "source": [
    "#standard deviation - 1 year volatility, all time volatility and rolling volatility\n",
    "#moving averages\n",
    "#percentage changes in price and volume\n",
    "#drawdown\n",
    "#correlation\n",
    "#need market index data for beta\n",
    "#volume rate of change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faebce37-d82e-467a-b9f8-91cbe1c4dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.set_option(\"compute.ops_on_diff_frames\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1dc588-e5d7-4208-9cba-d7996731f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:63: E251 unexpected spaces around keyword / parameter equals\n",
      "2:65: E251 unexpected spaces around keyword / parameter equals\n",
      "3:1: W391 blank line at end of file\n"
     ]
    }
   ],
   "source": [
    "psdf = df_cleaned.pandas_api()\n",
    "psdf_sorted = psdf.sort_values(by='datetime').reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da1a06f3-fc29-475a-a85e-2cf14c4d1e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:58: E251 unexpected spaces around keyword / parameter equals\n",
      "1:60: E251 unexpected spaces around keyword / parameter equals\n",
      "1:63: E231 missing whitespace after ','\n",
      "1:73: E251 unexpected spaces around keyword / parameter equals\n",
      "1:75: E251 unexpected spaces around keyword / parameter equals\n",
      "1:80: E231 missing whitespace after ','\n",
      "1:80: E501 line too long (96 > 79 characters)\n",
      "1:85: E251 unexpected spaces around keyword / parameter equals\n",
      "1:87: E251 unexpected spaces around keyword / parameter equals\n",
      "2:70: E251 unexpected spaces around keyword / parameter equals\n",
      "2:72: E251 unexpected spaces around keyword / parameter equals\n",
      "10:64: E225 missing whitespace around operator\n",
      "10:80: E501 line too long (95 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "psdf_sorted['ema'] = psdf_sorted['close'].ewm(min_periods = 12,ignore_na = True,span = 6).mean()\n",
    "psdf_sorted = psdf_sorted.sort_values(by='datetime').reset_index(drop = True)\n",
    "monthly_pct_change = psdf_sorted['close'].pct_change()\n",
    "monthly_pct_change = monthly_pct_change.round(3)\n",
    "psdf_sorted['monthly_pct_change'] = monthly_pct_change\n",
    "std = psdf_sorted['monthly_pct_change'].rolling(window=12).std()\n",
    "psdf_sorted['rolling_std_12_months'] = std\n",
    "sma = psdf_sorted['close'].rolling(window=12).mean()\n",
    "psdf_sorted['sma'] = sma\n",
    "vroc = (psdf_sorted['volume'] - psdf_sorted['volume'].shift(6))/ psdf_sorted['volume'].shift(6)\n",
    "psdf_sorted['vroc'] = vroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3deea339-a36e-4cba-b0b4-1c3f3b96f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:1: E265 block comment should start with '# '\n",
      "1:80: E501 line too long (183 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "#drawdown = (psdf_sorted['close'].rolling(window=6,min_periods = 6).min() - psdf_sorted['close'].rolling(window=6,min_periods = 6).max())/ psdf_sorted['close'].rolling(window=6).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdc1ef4c-3a48-453f-a2c4-482c098c6dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:1: E265 block comment should start with '# '\n"
     ]
    }
   ],
   "source": [
    "#psdf_sorted['drawdown'] = drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c19065f-4adc-4576-b9ee-a5097e9f9aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:65: E231 missing whitespace after ','\n",
      "1:77: E251 unexpected spaces around keyword / parameter equals\n",
      "1:79: E251 unexpected spaces around keyword / parameter equals\n",
      "1:80: E501 line too long (87 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "psdf_sorted['min value'] = psdf_sorted['close'].rolling(window=6,min_periods = 6).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79812da1-4d8a-40c0-82ea-7e56c8e39b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:65: E231 missing whitespace after ','\n",
      "1:77: E251 unexpected spaces around keyword / parameter equals\n",
      "1:79: E251 unexpected spaces around keyword / parameter equals\n",
      "1:80: E501 line too long (87 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "psdf_sorted['max value'] = psdf_sorted['close'].rolling(window=6,min_periods = 6).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fdc97cb-60a3-4b25-8d1f-3c208b377a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:1: E265 block comment should start with '# '\n",
      "2:1: E265 block comment should start with '# '\n",
      "2:80: E501 line too long (110 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "#wrap in a function\n",
    "#psdf_sorted['max drawdown'] = (psdf_sorted['min value'] - psdf_sorted['max value'])/ psdf_sorted['max value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56899eba-2bd9-49db-bc58-d1ff414cbcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf_sorted = psdf_sorted.sort_values(by='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3660b12-3b6b-4636-a088-0bf0e78f7a46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/23 17:24:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3303.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:101)\n\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:119)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:62)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000000701df92c8.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000000701df2690.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.runWith(Thread.java:1596)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/frame.py:13402\u001b[0m, in \u001b[0;36mDataFrame.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_display_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m  13400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_internal_pandas()\u001b[38;5;241m.\u001b[39mto_string()\n\u001b[0;32m> 13402\u001b[0m pdf \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_repr_pandas_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_display_count\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m  13403\u001b[0m pdf_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pdf)\n\u001b[1;32m  13404\u001b[0m pdf \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, pdf\u001b[38;5;241m.\u001b[39miloc[:max_display_count])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/frame.py:13393\u001b[0m, in \u001b[0;36mDataFrame._get_or_create_repr_pandas_cache\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m  13390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_or_create_repr_pandas_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries]:\n\u001b[1;32m  13391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache:\n\u001b[1;32m  13392\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\n\u001b[0;32m> 13393\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {n: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m  13394\u001b[0m         )\n\u001b[1;32m  13395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache[n]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/frame.py:13388\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_internal_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m  13383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  13384\u001b[0m \u001b[38;5;124;03m    Return a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m  13385\u001b[0m \n\u001b[1;32m  13386\u001b[0m \u001b[38;5;124;03m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m  13387\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 13388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/utils.py:600\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_lazy_property\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[0;32m--> 600\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/internal.py:1115\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1115\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1117\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m   1118\u001b[0m         {field\u001b[38;5;241m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m sdf\u001b[38;5;241m.\u001b[39mschema}\n\u001b[1;32m   1119\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3303.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:101)\n\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:119)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:62)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000000701df92c8.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000000701df2690.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.runWith(Thread.java:1596)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3338.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat scala.collection.mutable.ResizableArray.$init$(ResizableArray.scala:35)\n\tat scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:50)\n\tat scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:64)\n\tat scala.collection.mutable.ArrayBuffer$.newBuilder(ArrayBuffer.scala:195)\n\tat scala.collection.generic.GenericTraversableTemplate.newBuilder(GenericTraversableTemplate.scala:67)\n\tat scala.collection.generic.GenericTraversableTemplate.newBuilder$(GenericTraversableTemplate.scala:67)\n\tat scala.collection.AbstractTraversable.newBuilder(Traversable.scala:108)\n\tat scala.collection.mutable.BufferLike.clone(BufferLike.scala:262)\n\tat scala.collection.mutable.BufferLike.clone$(BufferLike.scala:261)\n\tat scala.collection.mutable.AbstractBuffer.clone(Buffer.scala:50)\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionSet.clone(ExpressionSet.scala:159)\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus(ExpressionSet.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.$anonfun$getAllValidConstraints$1(LogicalPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.$anonfun$getAllValidConstraints$1$adapted(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode$$Lambda/0x0000000701e89ed8.apply(Unknown Source)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.getAllValidConstraints(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.getAllValidConstraints$(LogicalPlan.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.getAllValidConstraints(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints$lzycompute(basicLogicalOperators.scala:92)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints(basicLogicalOperators.scala:91)\n\tat org.apache.spark.sql.catalyst.plans.logical.QueryPlanConstraints.constraints(QueryPlanConstraints.scala:34)\n\tat org.apache.spark.sql.catalyst.plans.logical.QueryPlanConstraints.constraints$(QueryPlanConstraints.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.constraints$lzycompute(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.constraints(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.Union.$anonfun$validConstraints$2(basicLogicalOperators.scala:535)\n\tat org.apache.spark.sql.catalyst.plans.logical.Union$$Lambda/0x0000000701e88bd8.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda/0x0000000701215568.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/frame.py:13422\u001b[0m, in \u001b[0;36mDataFrame._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_display_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m  13420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_internal_pandas()\u001b[38;5;241m.\u001b[39mto_html(notebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m> 13422\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_repr_pandas_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_display_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  13423\u001b[0m pdf_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pdf)\n\u001b[1;32m  13424\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39miloc[:max_display_count]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/frame.py:13393\u001b[0m, in \u001b[0;36mDataFrame._get_or_create_repr_pandas_cache\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m  13390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_or_create_repr_pandas_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries]:\n\u001b[1;32m  13391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache:\n\u001b[1;32m  13392\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\n\u001b[0;32m> 13393\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {n: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m  13394\u001b[0m         )\n\u001b[1;32m  13395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache[n]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/frame.py:13388\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_internal_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m  13383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  13384\u001b[0m \u001b[38;5;124;03m    Return a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m  13385\u001b[0m \n\u001b[1;32m  13386\u001b[0m \u001b[38;5;124;03m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m  13387\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 13388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/utils.py:600\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_lazy_property\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[0;32m--> 600\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/pandas/internal.py:1115\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1115\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1117\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m   1118\u001b[0m         {field\u001b[38;5;241m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m sdf\u001b[38;5;241m.\u001b[39mschema}\n\u001b[1;32m   1119\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3338.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat scala.collection.mutable.ResizableArray.$init$(ResizableArray.scala:35)\n\tat scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:50)\n\tat scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:64)\n\tat scala.collection.mutable.ArrayBuffer$.newBuilder(ArrayBuffer.scala:195)\n\tat scala.collection.generic.GenericTraversableTemplate.newBuilder(GenericTraversableTemplate.scala:67)\n\tat scala.collection.generic.GenericTraversableTemplate.newBuilder$(GenericTraversableTemplate.scala:67)\n\tat scala.collection.AbstractTraversable.newBuilder(Traversable.scala:108)\n\tat scala.collection.mutable.BufferLike.clone(BufferLike.scala:262)\n\tat scala.collection.mutable.BufferLike.clone$(BufferLike.scala:261)\n\tat scala.collection.mutable.AbstractBuffer.clone(Buffer.scala:50)\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionSet.clone(ExpressionSet.scala:159)\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus(ExpressionSet.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.$anonfun$getAllValidConstraints$1(LogicalPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.$anonfun$getAllValidConstraints$1$adapted(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode$$Lambda/0x0000000701e89ed8.apply(Unknown Source)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.getAllValidConstraints(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.getAllValidConstraints$(LogicalPlan.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.getAllValidConstraints(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints$lzycompute(basicLogicalOperators.scala:92)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints(basicLogicalOperators.scala:91)\n\tat org.apache.spark.sql.catalyst.plans.logical.QueryPlanConstraints.constraints(QueryPlanConstraints.scala:34)\n\tat org.apache.spark.sql.catalyst.plans.logical.QueryPlanConstraints.constraints$(QueryPlanConstraints.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.constraints$lzycompute(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.constraints(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.Union.$anonfun$validConstraints$2(basicLogicalOperators.scala:535)\n\tat org.apache.spark.sql.catalyst.plans.logical.Union$$Lambda/0x0000000701e88bd8.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda/0x0000000701215568.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n"
     ]
    }
   ],
   "source": [
    "psdf_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad6294-c52d-4a8f-a089-173f907c401c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
