{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe56fb-00f7-448a-91cb-441fd2185c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "# %pycodestyle_off -to turn it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54cb767a-7b5a-4cba-b24d-0ae3feded6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, month, year,monotonically_increasing_id,row_number,concat, udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType,TimestampNTZType,StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.conf import SparkConf\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from binance import Client\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from binance.helpers import date_to_milliseconds, interval_to_milliseconds\n",
    "from binance.exceptions import BinanceRequestException, BinanceAPIException\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dateutil.parser import parse\n",
    "import time\n",
    "import logging\n",
    "import psycopg2\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e599f2a-a8c2-4d5b-8acc-6550f36bd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "MINIO_USER = os.getenv(\"MINIO_ROOT_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "client_binance = Client(API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4023de86-2b14-4e29-90e0-64a949ce864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/24 17:45:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/24 17:45:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CryptoETL\") \\\n",
    "    .config(\"spark.jars\", \"/Users/hamza/Desktop/projects/postgresql-42.7.5.jar\") \\\n",
    "    .getOrCreate()\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", MINIO_USER)#turn into access key in the future \n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", MINIO_PASSWORD)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe1b0bb-56a4-438f-8801-796b493aaefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data for 1 crypto\n",
    "#schema check/validation need to do this, create spark df using schema struct\n",
    "#implement transformations\n",
    "#add unique id for crypto,price,time\n",
    "#split into 3 tables\n",
    "#upload to postgres db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ad682-a432-4de4-b526-4331b68563fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) #set to debug to capture all levels\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847fe01e-ba2f-417d-8d7e-41fbfaffedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5dce917-2573-401d-918a-13f4bd641df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptos = ['BTCUSDT','ETHUSDT','LTCUSDT','BNBUSDT','DOGEUSDT']\n",
    "client_minio = Minio(\n",
    "        \"localhost:9000\",  # Make sure you're using port 9000 for the S3 API\n",
    "        #minio_url,\n",
    "        access_key = MINIO_USER,\n",
    "        secret_key = MINIO_PASSWORD,\n",
    "        secure=False  # Disable SSL if you're not using SSL certificates\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56649b2f-067a-4555-aec5-222df250b133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "objects = client_minio.list_objects(\"binancedata\", prefix=\"BNBUSDT\", recursive=True)\n",
    "filenames = [obj.object_name for obj in objects]\n",
    "filenames = [f for f in filenames if \"_SUCCESS\" not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14343e93-3bde-4570-a2c1-97da82850e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/24 17:46:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_bnb = spark.read.parquet(\"s3a://binancedata/BNBUSDT/Monthly/year=2019/month=10/part-00001-12233d6e-b55f-4e5a-8285-808d25f4bf53.c000.snappy.parquet\")\n",
    "for file in filenames:\n",
    "    df = spark.read.parquet(f\"s3a://binancedata/{file}\")\n",
    "    df_bnb = df_bnb.union(df)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57486214-bb03-4e29-b0ab-835f59da981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning():\n",
    "    pass\n",
    "    #drop duplicates\n",
    "    #convert data types\n",
    "    #drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5d9eb-d21a-4e27-bd25-829d60f0beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_table():\n",
    "    pass\\\n",
    "    #read from postgres\n",
    "    #convert table to df and join with dataframe\n",
    "    #check if id\n",
    "    #convert dataframe to sql tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0db87-107d-42fc-a0d4-1fbeca978182",
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_dict = [{\"name\":\"Bitcoin\",\"ticker\":\"BTC\"},\n",
    "               {\"name\":\"Ethereum\",\"ticker\":\"ETH\"},\n",
    "               {\"name\":\"Litecoin\",\"ticker\":\"LTC\"},\n",
    "               {\"name\":\"Dogecoin\",\"ticker\":\"DOGE\"},\n",
    "               {\"name\":\"BNB\",\"ticker\":\"BNB\"}\n",
    "              ]\n",
    "crypto_df = spark.createDataFrame(crypto_dict)\n",
    "#crypto_df = crypto_df.withColumn('id',monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41258e2-2ec0-45b9-b828-0dea17c7dcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crypto_df.write     .format(\"jdbc\")     .option(\"url\", \"jdbc:postgresql://localhost:5432/crypto\")     .option(\"dbtable\", \"crypto\")     .option(\"user\", \"postgres\")     .option(\"password\", \"postgres\")     .option(\"driver\", \"org.postgresql.Driver\")     .mode(\"append\")    .save()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''crypto_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/crypto\") \\\n",
    "    .option(\"dbtable\", \"crypto\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\")\\\n",
    "    .save()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5852cf9-c1f9-4fcd-87d5-f5ddbf52f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_sql = \"SELECT * FROM Crypto\"\n",
    "\n",
    "df_crypto = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/crypto\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .option(\"query\", read_sql)\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46e90ee-ca1e-4866-9999-de7fde07b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crypto = df_crypto.withColumn(\"trading pair\", concat(df_crypto.ticker, lit(\"USDT\")))\n",
    "df_crypto = df_crypto.filter(col(\"trading pair\") == \"BNBUSDT\")\n",
    "crypto_id = df_crypto.collect()[0]['id']\n",
    "df_bnb = df_bnb.withColumn(\"crypto_id\",lit(crypto_id)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc37cf95-c47f-47f7-8b9c-66709abb0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bnb = df_bnb.withColumn(\"time_id\",int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d753792-3651-45c8-a172-422230fc8681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----------+------------------+---------+------------+\n",
      "|           datetime| Open Price|Close Price|            Volume|crypto_id|     time_id|\n",
      "+-------------------+-----------+-----------+------------------+---------+------------+\n",
      "|2019-10-01 01:00:00|15.84930000|19.90990000| 53481014.91000000|        5|           0|\n",
      "|2019-01-01 00:00:00| 6.11390000| 6.22000000| 72126339.77000000|        5|  8589934592|\n",
      "|2019-10-01 01:00:00|15.84930000|19.90990000| 53481014.91000000|        5| 17179869184|\n",
      "|2019-11-01 00:00:00|19.90870000|15.71180000| 46441747.10000000|        5| 25769803776|\n",
      "|2019-12-01 00:00:00|15.70300000|13.71610000| 38189582.55000000|        5| 34359738368|\n",
      "|2019-02-01 00:00:00| 6.22500000|10.27790000|110685041.86000000|        5| 42949672960|\n",
      "|2019-03-01 00:00:00|10.27770000|17.53760000|115379333.44000000|        5| 51539607552|\n",
      "|2019-04-01 01:00:00|17.52010000|22.03910000| 83676534.62000000|        5| 60129542144|\n",
      "|2019-05-01 01:00:00|22.03000000|32.88620000|104145121.86000000|        5| 68719476736|\n",
      "|2019-06-01 01:00:00|32.91600000|32.11400000| 93155800.74000000|        5| 77309411328|\n",
      "|2019-07-01 01:00:00|32.11410000|27.75000000| 71835825.11000000|        5| 85899345920|\n",
      "|2019-08-01 01:00:00|27.73480000|21.06040000| 43730003.73000000|        5| 94489280512|\n",
      "|2019-09-01 01:00:00|21.06020000|15.83490000| 44740437.45000000|        5|103079215104|\n",
      "|2020-01-01 00:00:00|13.71590000|18.26440000| 51701909.53000000|        5|111669149696|\n",
      "|2020-10-01 01:00:00|29.26960000|28.50580000| 81019994.54300000|        5|120259084288|\n",
      "|2020-11-01 00:00:00|28.50580000|31.49350000| 80529464.34400000|        5|128849018880|\n",
      "|2020-12-01 00:00:00|31.49350000|37.35880000| 81889998.06000000|        5|137438953472|\n",
      "|2020-02-01 00:00:00|18.26440000|19.23000000| 80993103.39000000|        5|146028888064|\n",
      "|2020-03-01 00:00:00|19.24810000|12.54510000|137539733.73000000|        5|154618822656|\n",
      "|2020-04-01 01:00:00|12.54970000|16.95420000|118041230.55000000|        5|163208757248|\n",
      "+-------------------+-----------+-----------+------------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bnb.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae3dc860-b03f-42a9-a75f-20ea3e32cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_id(dt_value):\n",
    "    ts = dt_value.strftime(\"%Y%m\")\n",
    "    return int(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be199492-aac8-4bbe-a02f-7df36587e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_udf = udf(generate_time_id,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b211fd68-6c2d-49f3-8f12-4bbfe1fda35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_udf = df_bnb.withColumn(\"time_id\", dt_udf(df_bnb[\"datetime\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a87b60b-b391-4b5d-b214-18ad9b8466be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----------+-----------------+---------+-------+\n",
      "|           datetime| Open Price|Close Price|           Volume|crypto_id|time_id|\n",
      "+-------------------+-----------+-----------+-----------------+---------+-------+\n",
      "|2019-10-01 01:00:00|15.84930000|19.90990000|53481014.91000000|        5| 201910|\n",
      "|2019-01-01 00:00:00| 6.11390000| 6.22000000|72126339.77000000|        5| 201901|\n",
      "|2019-10-01 01:00:00|15.84930000|19.90990000|53481014.91000000|        5| 201910|\n",
      "|2019-11-01 00:00:00|19.90870000|15.71180000|46441747.10000000|        5| 201911|\n",
      "|2019-12-01 00:00:00|15.70300000|13.71610000|38189582.55000000|        5| 201912|\n",
      "+-------------------+-----------+-----------+-----------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_udf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0575ca3-1e0e-4260-8c8d-dd5270d13808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_udf = df_with_udf.withColumn(\"year\", year(df_with_udf[\"datetime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ac20066-0435-4826-bda0-9c6435b840cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_udf = df_with_udf.withColumn(\"month\", month(df_with_udf[\"datetime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88b7f3ba-2b2a-43c2-b66e-d48073bb3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_with_udf.select(['crypto_id','time_id','year','month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d90c0201-d88d-4b11-ae45-afc095108fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_time.withColumnRenamed('time_id','id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4031646-33ad-44b5-944a-40e89f13c417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----+-----+\n",
      "|crypto_id|    id|year|month|\n",
      "+---------+------+----+-----+\n",
      "|        5|201910|2019|   10|\n",
      "|        5|201901|2019|    1|\n",
      "|        5|201910|2019|   10|\n",
      "|        5|201911|2019|   11|\n",
      "|        5|201912|2019|   12|\n",
      "+---------+------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecf79352-ec9a-4f61-b6d7-556e7beb2c6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/24 18:54:17 ERROR Executor: Exception in task 0.0 in stage 109.0 (TID 552)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "25/04/24 18:54:17 ERROR TaskSetManager: Task 0 in stage 109.0 failed 1 times; aborting job\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 14.0 in stage 109.0 (TID 566) (hamzas-laptop executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 10.0 in stage 109.0 (TID 562) (hamzas-laptop executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 11.0 in stage 109.0 (TID 563) (hamzas-laptop executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 12.0 in stage 109.0 (TID 564) (hamzas-laptop executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 13.0 in stage 109.0 (TID 565) (hamzas-laptop executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 9.0 in stage 109.0 (TID 561) (hamzas-laptop executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 8.0 in stage 109.0 (TID 560) (hamzas-laptop executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/04/24 18:54:17 WARN TaskSetManager: Lost task 15.0 in stage 109.0 (TID 567) (hamzas-laptop executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n",
      "  Detail: Key (id)=(201910) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o511.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n  Detail: Key (id)=(201910) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)\n\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n  Detail: Key (id)=(201910) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_time\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://localhost:5432/crypto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DE-Crypto-aeKJmBel/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o511.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 552) (hamzas-laptop executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n  Detail: Key (id)=(201910) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)\n\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO Time (\"crypto_id\",\"id\",\"year\",\"month\") VALUES (('5'::int4),('201910'::int4),('2019'::int4),('10'::int4)) was aborted: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n  Detail: Key (id)=(201910) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:889)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:913)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"time_pkey\"\n  Detail: Key (id)=(201910) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "df_time.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/crypto\") \\\n",
    "    .option(\"dbtable\", \"Time\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ac00b-006e-4733-b067-7d9f0f0de71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefine your crypto tables\n",
    "#apply functional programming\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
